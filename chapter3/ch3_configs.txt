Running a simple MapReduce Program:

5.
$ yarn jar /opt/cluster/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar

6.

$ yarn jar /opt/cluster/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /input_file /out_directory


Hadoop streaming:

3.

#!/usr/bin/env python

import sys

# input comes from STDIN (standard input)
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()
    # split the line into words
    words = line.split()
    # increase counters
    for word in words:
        # write the results to STDOUT (standard output);
        # what we output here will be the input for the
        # Reduce step, i.e. the input for reducer.py
        #
        # tab-delimited; the trivial word count is 1
print '%s\t%s' % (word, 1)

4.

#!/usr/bin/env python

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# input comes from STDIN
for line in sys.stdin:
    # remove leading and trailing whitespace
    line = line.strip()

    # parse the input we got from mapper.py
    word, count = line.split('\t', 1)

   # convert count (currently a string) to int
    try:
        count = int(count)
    except ValueError:
        # count was not a number, so silently
        # ignore/discard this line
        continue

    # this IF-switch only works because Hadoop sorts map output
    # by key (here: word) before it is passed to the reducer
    if current_word == word:
        current_count += count
    else:
        if current_word:
            # write result to STDOUT
            print '%s\t%s' % (current_word, current_count)
        current_count = count
        current_word = word

# do not forget to output the last word if needed!
if current_word == word:
print '%s\t%s' % (current_word, current_count)


Configure Yarn History Server:

4. yarn-site.xml

<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>

5.

<property>
    <name>mapreduce.jobhistory.address</name>
    <value>master1.cyrus.com:10020</value>
</property>

6.

<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>master1.cyrus.com:19888</value>
</property>

7.

<property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/tmp/logs</value>
</property>

Configure ResourceManager Components

3.

<property>
<name>yarn.resourcemanager.resource-tracker.address.rm1</name>
<value>master1.cyrus.com:9001</value>
</property>

<property>
<name>yarn.resourcemanager.scheduler.address.rm1</name>
<value>master1.cyrus.com:9002</value>
</property>

<property>
<name>yarn.resourcemanager.address.rm1</name>
<value>master1.cyrus.com:9003</value>
</property>


Yarn Containers and Resource Allocations
	
4.

<property>
<name>yarn.nodemanager.resource.memory.mb</name>
<value>11264</value>
</property>

5. 

<property>
<name>yarn.app.mapreduce.am.resource.mb</name>
<value>1536</value>
</property>

6.

<property>
    <name>yarn.scheduler.increment-allocation-mb</name>
    <value>32</value>
</property>

7.

<property>
<name>yarn.scheduler.maximum-allocation-mb</name>
<value>11264</value>
</property>

8.

<property>
<name>yarn.scheduler.minimum-allocation-vcores</name>
<value>1</value>
</property>

<property>
<name>yarn.scheduler.maximum-allocation-vcores</name>
<value>16</value>
</property>

9.

<property>
<name>yarn.nodemanager.vmem-pmem-ratio</name>
<value>3.0</value>
</property>

<property>
<name>yarn.nodemanager.pmem-check-enabled</name>
<value>true</value>
</property>

<property>
<name>yarn.nodemanager.vmem-check-enabled</name>
<value>true</value>
</property>

There's more:

mapred-site.xml

<property>
<name>mapreduce.map.memory.mb</name>
<value>1536</value>
</property>

<property>
<name>mapreduce.reduce.memory.mb</name>
<value>2048</value>
</property>

<property>
<name>mapreduce.map.java.opts</name>
<value>-Xmx1152m</value>
</property>

<property>
<name>mapreduce.reduce.java.opts</name>
<value>-Xmx1280m</value>
</property>

<property>
<name>mapreduce.job.counters.max</name>
<value>200</value>
</property>


Preserving ResourceManager States

4.

<property>
<name>yarn.resourcemanager.recovery.enabled</name>
<value>true</value>
</property>

5.

<property>
<name>yarn.resourcemanager.store.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</value>
</property>

6.

<property>
<name>yarn.resourcemanager.fs.state-store.uri</name>
<value>hdfs://localhost:9000/rmstore</value>
</property>
